{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db305d7-3f18-43b8-a961-0cfa79284639",
   "metadata": {},
   "source": [
    "# Chapter 7 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f80642-11b4-4bf5-a263-32b920a92b72",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae63e7c-cc00-481d-beab-b3588fe3c7f9",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27e73b3-2186-43cf-b7ce-35a80ce058ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import arviz as az\n",
    "import graphviz as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.interpolate import BSpline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f5cae-1e85-4013-878e-78ce3b9de9ce",
   "metadata": {},
   "source": [
    "### Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61fe68b-59fc-4915-90e0-1e4a4d8f7a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seaborn defaults\n",
    "sns.set(\n",
    "    style=\"whitegrid\",\n",
    "    font_scale=1.2,\n",
    "    rc={\n",
    "        \"axes.edgecolor\": \"0\",\n",
    "        \"axes.grid.which\": \"both\",\n",
    "        \"axes.labelcolor\": \"0\",\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"xtick.bottom\": True,\n",
    "        \"ytick.left\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "colors = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d545d-8886-40e4-b6b6-52d52d50e5af",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a09aea4-58d5-44f5-9267-160437fb6d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "HOWELL_FILE = \"howell.csv\"\n",
    "CHERRY_BLOSSOMS_FILE = \"cherry_blossoms.csv\"\n",
    "WAFFLE_DIVORCE_FILE = \"waffle_divorce.csv\"\n",
    "MILK_FILE = \"milk.csv\"\n",
    "LDS_FILE = \"lds_by_state.csv\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebbac1ab-add4-4555-962f-e4a500b084a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_name, data_dir=DATA_DIR, **kwargs):\n",
    "    path = os.path.join(data_dir, file_name)\n",
    "    return pd.read_csv(path, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f5d9d-208e-430c-b7b2-dd43bb7f1650",
   "metadata": {},
   "source": [
    "## Easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9185b89-e103-4477-967e-0d66b42a5bfd",
   "metadata": {},
   "source": [
    "### 7E1\n",
    "\n",
    "State the three motivating criteria that define information entropy.\n",
    "Try to express each in your own words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842635c3-b6b7-4c4c-bf70-b30afa25a676",
   "metadata": {},
   "source": [
    "Information entropy is a function of probabilities $p=(p_i)$ such that the following hold.\n",
    "1. It is continuous in the $p_i$.\n",
    "2. Increasing the number of possible outcomes should increase the entropy. That is, if a possible outcome with probability $p_i$ is replaced by two outcomes whose probabilities sum to $p_i$ then the entropy should increase (strictly if the new outcomes have non-zero probability)\n",
    "3. It is additive, in the sense that if we add a new *independent* random variable then then entropy of the joint distribution is the sum of the entropies of the marginal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e69ee-291c-4e1e-b31b-31467cc01d3b",
   "metadata": {},
   "source": [
    "### 7E2\n",
    "\n",
    "Suppose a coin is weighted such that, when it is tosssed and lands on a table, it comes up heads 70% of the time.\n",
    "What is the entropy of the coin.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c1dcf4c-bc9a-4372-adf6-a17f24a4d618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def information_entropy(prob):\n",
    "    prob = np.array(prob)\n",
    "    return - np.sum(prob * np.log(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c726209-0dbd-4578-9afa-f10f3a4e97d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6108643020548935"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_entropy([0.3, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50dd17-d795-458a-b044-d23001674b54",
   "metadata": {},
   "source": [
    "### 7E3\n",
    "\n",
    "suppose a four-sided die is loaded such that, when tossed onto a table, it shows \"1\" 20%, \"2\" 25%, \"3\" 25%, and \"4\" 30% of the time.\n",
    "What is the entropy of the die?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99baa1cd-4d17-4df2-91b4-4fb04266d940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3762266043445461"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_entropy([0.2, 0.25, 0.25, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b370e96-6ff6-4ceb-ba0d-fb3e8419d19d",
   "metadata": {},
   "source": [
    "### 7E4\n",
    "\n",
    "Suppose another four-sided die is loaded such that it never shows \"4\".\n",
    "The other three sides show equally often.\n",
    "What is the entropy of this die.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3911c4-c59e-474b-aeb7-1691f3d204e7",
   "metadata": {},
   "source": [
    "We don't need to account for events with probability zero, so the entropy is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16d903c8-70b9-47e4-9482-9cceb3cf30a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681096"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_entropy([1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b105392-c09c-4079-a483-c4b21b4d3da2",
   "metadata": {},
   "source": [
    "## Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bcc4d-2fc7-4121-882c-60e2f18c7d1b",
   "metadata": {},
   "source": [
    "### 7M1\n",
    "\n",
    "Write down and compare the definitions of AIC and WAIC.\n",
    "Which of these criteria is most generic?\n",
    "Which assumptions are required to transform the more general criterion into a less general one?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91ffb1-ef08-4f74-997f-61de6e775efd",
   "metadata": {},
   "source": [
    "The criteria are defined by\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{AIC} &= D_{\\text{train}} + 2p, & \\text{WAIC} &= D_{\\text{train}} + 2\\sum_i \\text{Var}(\\ln(\\text{Pr}(y_i | \\Theta))),\n",
    "\\end{align}\n",
    "\n",
    "where $D_{\\text{train}}$ is the devaince over the training set, $p$ is the number of parameters, $y_i$ are the observations, and $\\Theta$ is the set of parameters.\n",
    "The difference between the definitions is in the correction term - the expression for WAIC is a more accurate estimate of the over-fitting bias in the training deviance.\n",
    "\n",
    "The WAIC is the most generic (hence WI = Widely Applicable).\n",
    "\n",
    "I don't quite understand the last part of the question, but I think it's asking under what assumptions do these definitions agree.\n",
    "We need to assume\n",
    "1. Flat priors\n",
    "2. Multivariate normal posteriors\n",
    "3. The sample size is much greater than the number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cb9dd-d87a-4383-9516-34f7a293db62",
   "metadata": {},
   "source": [
    "### 7M2\n",
    "\n",
    "Explain the difference between model *selection* and model *comparison*.\n",
    "What information is lost under model selection?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f582cb-b14f-4515-a083-b3a592f123aa",
   "metadata": {},
   "source": [
    "Model selection means building multiple models and selecting which to use based on some estimate of out-of-sample predictive accuracy, such as WAIC or PSIS.\n",
    "Essentially it is using WAIC or PSIS to identify a 'best' model.\n",
    "Model comparison means using WAIC or PSIS to understand how different variables (and modelling assumptions) influence predictions and predictive accuracy.\n",
    "\n",
    "Under model selection we lose the context of the models considered, and their relative scores.\n",
    "We also disregard casual questions when we focus solely on predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2867598-70b0-47d4-a3a2-3a714be6440e",
   "metadata": {},
   "source": [
    "### 7M3\n",
    "\n",
    "When comparing models with an information criterion, why must all models be fit to exactly the same observations?\n",
    "What would happen to the information criterion values, if the models were fit to different numbers of observations?\n",
    "Perform some experiments, if you are not sure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b8ebd-c345-47f2-9efc-c2b2d9cb36e2",
   "metadata": {},
   "source": [
    "The information criteria defined are explicitly dependent on the observations.\n",
    "In particular, they are additive - the value for a group of observations is the sum of the individual values.\n",
    "This means that expanding a sample of observations can only increase the value of the information criterion.\n",
    "This can be solved by taking an average; that is, dividing the number of information criterion by the number of samples.\n",
    "This should theoretically make the information criterion independent of the observations for a sufficiently large sample, but in practice particular observations may have a large effect on the criterion and so it's important that these be included for all models being compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d108e8-95a4-4511-a0e0-3d868be305f0",
   "metadata": {},
   "source": [
    "### 7M4\n",
    "\n",
    "What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated?\n",
    "Why?\n",
    "Perform some experiments, if you are not sure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73197d86-671f-4884-a3aa-da41d058d470",
   "metadata": {},
   "source": [
    "With a completely flat prior all observations are equally plausible.\n",
    "Thus the log-likelihood of the observations is relatively flat (even under the posterior distributions).\n",
    "As the prior becomes more opinionated, there is more variance in the likelihood of different observations and so the number of effective parameters in WAIC decreases.\n",
    "Since importance sampling involves weighting observations by the reciprocals of their (posterior) likelihoods, again higher variance in the likelihoods of different observations means that we give (relatively) more weight to lower likelihoods which decreases PSIS.\n",
    "\n",
    "This also makes intuitive sense; the number of effective parameters quantifies how the extent to which the model is overfit and so tighter priors should decrease the value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767f0dd-3b62-4129-afd1-d5a1de1bc6f2",
   "metadata": {},
   "source": [
    "### 7M5\n",
    "\n",
    "Provide an informal explanation of why informative priors reduce overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50fc02-097f-4640-802e-8b27158d44a0",
   "metadata": {},
   "source": [
    "Informative priors reduce the extent to which individual observations influence the likelihood.\n",
    "For example, if I have an extremely uninformative prior on human height and then measure the heights of a few basketball players, it will lead to a strong belief that 2 metres is an normal height, and so my model will be bad at predicting the heights of the general population.\n",
    "However if I use an informative prior my model won't be so affected by these extreme values and will do better in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e5ebb-b5d0-491b-b6ce-cb3c70f56a4f",
   "metadata": {},
   "source": [
    "### 7M6\n",
    "\n",
    "Provide an informal explanation of why overly informative priors result in underfitting.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b519c6b-6849-4cd4-ad18-597c81080978",
   "metadata": {},
   "source": [
    "Overly informative priors dominate the observed data meaning that your posterior is relatively similar to the prior.\n",
    "This means that the model doesn't learn much from the data, including the true (non-stochastic) patterns, and so will generalise less well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40735ae8-85fe-4625-9097-90ee52df32d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
